---
layout: home
title: Learning robust autonomous navigation and locomotion for wheeled-legged robots
author: Joonho Lee
---

<figure class="figure" id="fig1">
    <img alt="swissmile-robot-highlights" src="assets/img/figure1.png" class="figure">
</figure>

<div class="paper-title">
    {{ page.title }}
</div>


<div class="paper-authors">
    <a href="https://scholar.google.com/citations?user=6Htb7swAAAAJ&hl=en">Joonho Lee<sup>1,2</sup></a>, 
    <a href="https://scholar.google.com/citations?user=W7PnUfUAAAAJ&hl=en">Marko Bjelonic<sup>1,3</sup></a>, 
    <a href="https://www.linkedin.com/in/alexander-reske">Alexander Reske<sup>1,3</sup></a>, 
    <a href="https://scholar.google.com/citations?user=0otNjsQAAAAJ&hl=en&oi=ao">Lorenz Wellhausen<sup>1,3</sup></a>, 
    <a href="https://scholar.google.com/citations?user=nOl83tYAAAAJ&hl=en">Takahiro Miki<sup>1</sup></a>,
    and <a href="https://scholar.google.ch/citations?user=DO3quJYAAAAJ&hl=en">Marco Hutter<sup>1</sup></a>
</div>

<div class="paper-affiliations">
    <sup>1</sup> Robotic Systems Lab, ETH Zurich, Zurich, Switzerland
    <br><sup>2</sup> Neuromeka, Seoul, Korea
    <br><sup>3</sup> Swiss-Mile Robotics AG, Zurich, Switzerland
    <br>
    <em>
        <br>Substantial part of the work was carried out during their stay at 1
        <br><sup>*</sup> Corresponding author: joonho.lee@neuromeka.com
    </em>
</div>


<!-- <h3>Paper links</h3>

<span style="color:#707070">
    <p class="paper-paragraph">
        <a href="https://robotics.sciencemag.org/content/5/47/eabc5986">Science Robotics Vol.5 eabc5986 (2020)</a>
        <br>Author's version <a href="https://arxiv.org/abs/2010.11251">PDF</a>
    </p></span> -->

<hr class="thick2">

<h2 id="abs">ABSTRACT</h2>
<p class="paper-paragraph">
    Autonomous wheeled-legged robots have the potential to transform logistics systems, improving  operational efficiency and adaptability in urban environments.
    Navigating urban environments,however, poses unique challenges for robots, necessitating innovative solutions for locomotion and navigation. 
    These challenges include the need for adaptive locomotion across varied terrains and the ability to navigate efficiently around complex dynamic obstacles. 
    This work introduces a fully integrated system comprising adaptive locomotion control, mobility-aware local navigation planning, and large-scale path planning within the city.
    Using model-free reinforcement learning (RL) techniques and privileged learning, we develop a versatile locomotion controller. 
    This controller achieves efficient and robust locomotion over various rough terrains, facilitated by smooth transitions between walking and driving modes.
    It is tightly integrated with a learned navigation controller through a hierarchical RL framework, enabling effective navigation through challenging terrain and various obstacles at high speed.
    Our controllers are integrated into a largescale urban navigation system and validated by autonomous, kilometer-scale navigation missions conducted in Zurich, Switzerland, and Seville, Spain.
    These missions demonstrate the systemâ€™s robustness and adaptability, underscoring the importance of integrated control systems in
    achieving seamless navigation in complex environments. Our findings support the feasibility of
    wheeled-legged robots and hierarchical RL for autonomous navigation, with implications for lastmile delivery and beyond.
</p>


<hr class="thick">
<figcaption>
    Summary of main contributions
</figcaption>

<div class="videoWrapper" id="movie1">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/9j2a1oAHDL8" frameborder="0"
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
            allowfullscreen></iframe>
</div>


<div>
    <hr class="thick2">
    <h2 id="featured">Highlights</h2>
    <div style=" padding-bottom: 1rem;">
            <em><h3>Kilometer-scale Autonomous Deployments</h3></em>
        </div>

    <figcaption>
        Large scale autonomous missions in Glattpark, Zurich, Switzerland
        <span style="font-size: 1rem; color:#595959">(paper Figure 3)</span>
    </figcaption>
    <figure class="figure" id="fig2">
        <img alt="figure2" src="assets/img/glatt_park_A.png">
    </figure>


    <p class="paper-paragraph">
       <b>(A)</b> Our city navigation workflow begins with offline preparation, involving scanning the test area using a handheld laser scanner and constructing a navigation graph.
       <b>(B)</b> The robot autonomously navigated the urban environment to reach 13 predetermined goal points, selected in an arbitrary order. 
       <b>(i and ii)</b> Path planning within the city was facilitated by the pre-generated navigation graph.
       <b>(iii)</b>Moving speed and mechanical cost of transport compared to a normal legged robot (ANYmal-C).
</p>
    <!-- <p class="paper-paragraph">
        The presented controller has been deployed in diverse natural environments.
        These include steep mountain trails, creeks with running water, mud, thick vegetation, loose rubble,
        snow-covered
        hills, and a damp forest.
        A number of specific scenarios are further highlighted in <a href="#fig2">Fig. 1</a>A-F. These environments have
        characteristics that the policy does not experience during training. The terrains can deform and crumble, with
        significant variation of material properties over the surface. The robot's legs are subjected to frequent
        disturbances due to vegetation, rubble, and sticky mud.
        Existing terrain estimation pipelines that use cameras or LiDAR fail in environments
        with
        snow (<a href="#fig2">Fig. 1</a>A), water (<a href="#fig2">Fig. 1</a>C),
        or dense vegetation (<a href="#fig2">Fig. 1</a>F).
        Our controller does not rely on exteroception and is immune to such failure.
        The controller learns omnidirectional locomotion based on a history of proprioceptive observations and
        is robust in zero-shot deployment on terrains with characteristics that were never experienced during training.
    </p>

    <p class="paper-paragraph">
        Our controller was used by the Cerberus team for the DARPA Subterranean Challenge Urban Circuit
        (<a href="#fig2">Fig. 1</a>G).
        It replaced a model-based controller that had been employed used by the team in the past.
        The objective of the competition is to develop robotic systems that rapidly map, navigate,
        and search complex underground environments, including tunnels, urban underground, and cave
        networks. The human operators are not allowed to assist the robots during the competition physically;
        only teleoperation is allowed. Accordingly, the locomotion controller needs to perform without failure
        over extended mission durations.
        The presented controller drove two ANYmal-B robots in four missions of 60 minutes. The
        controller exhibited a zero failure rate throughout the competition. A steep staircase that was traversed
        by one of the robots during the competition is shown in <a href="#fig2">Fig. 1</a>G.
    </p> -->
    
    <hr class="thick">
    
    <div style=" padding-bottom: 1rem;">
    <em><h3>Local Navigation</h3></em>
    </div>

    <figcaption>
        Obstacle negotiation with active exploration
        <span style="font-size: 1rem; color:#595959">(paper Movie S2)</span>
    </figcaption>
    <div class="videoWrapper" id="movieS3">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/tPixnjLbTvE" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>


    <figure class="figure" id="fig2">
        <img alt="figure3" src="assets/img/obstacles.png">
    </figure>

    <p class="paper-paragraph">
        <b>(A)</b> Our robot navigates around blocked routes by actively exploring the area and finding
        alternative paths.
        <b>(B)</b> Safe traversal of a narrow space.
        <b>(C)</b>Our robot exhibits two different ways to traverse the complex obstacle. 
        <b>(C and D)</b> Our robot shows an asymmetric understanding of traversability, being able to traverse higher steps when going down.
        <b>(E)</b> We ensure safety around humans by incorporating additional human detection and overriding height scan values.
    </p>
    
    <hr class="thick">

    <div style=" padding-bottom: 1rem;">
    <em><h3>Terrain-adaptive Hybrid Locomotion</h3></em>
    </div>

    <figcaption>
        Locomotion highlights
        <span style="font-size: 1rem; color:#595959">(paper Movie S4)</span>
    </figcaption>
    <div class="videoWrapper" id="movieS4">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/3Nr47MXCFO0" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <p class="paper-paragraph">
       With model-free RL, we could develop a versatile locomotion controller that manifests adaptive gait transitions between walking and driving modes depending on the terrain.
    </p>

    <!-- <hr class="thick">

    <div style=" padding-bottom: 1rem;">
    <em><h3>Robustness to Foot Slippage</h3></em>
    </div>
    <figcaption>
        Movie 3. Foot Slippage experiment
        <span style="font-size: 1rem; color:#595959">(paper Movie S5)</span>
    </figcaption>
    <figure class="videoWrapper">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/aMPwB3t4idU" frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </figure>

    <p class="paper-paragraph">
        Next we test robustness to foot slippage.
        To introduce slippage, we used a moistened whiteboard.
        The results are shown in <a href="#movieS5">Movie 3</a>.
        The baseline quickly loses balance, aggressively swings the legs, and falls. In contrast, the presented
        controller adapts to the slippery terrain and successfully locomotes in the commanded direction.
    </p> -->

</div>
<p>

</p>
<hr class="thick2">
<div id="acknowlege">
    <h2>Acknowledgment</h2>
    <p class="paper-paragraph">
        <b>Author contributions</b>: J.L. conceived the main idea for the approach and was responsible for the implementation and training of the controllers.
        The high-level policy was trained by J.L., whereas M.B. developed the simulation environment for the low-level policy and also trained it. 
        The navigation system and safety layer were collaboratively devised and implemented by J.L., M.B.,A.R., and L.W. 
        Real-world experiments were planned and executed with contributions from A.R. and L.W.
        Initial setup of the low-level controller was facilitated by T.M. All authors contributed to system integration and experimentation.
    </p>
    <p class="paper-paragraph">
        <b>Thanks to </b>: We appreciate T. Tuna for helping us integrate Open3D SLAM, J. Keller for the API integration, and G. Valsecchi for the hardware support.
    </p>
    <p class="paper-paragraph">
    <b>Funding</b>: This work was supported by the Mobility Initiative grant funded through the ETH Zurich Foundation, European Unionâ€™s Horizon 2020 research and innovation program under grant
     agreement numbers 101070405 and 101016970, Swiss National Science Foundation through the National Centre of Competence in Digital Fabrication (NCCR dfab),
      European Unionâ€™s Horizon Europe Framework Programme under grant agreement numbers 852044 and 101070596, and Apple Inc.
        <!--        <b>Competing interests</b>: The authors declare that they have no competing-->
        <!--        interests.  <b>Data and materials availability</b>: All data needed to evaluate the conclusions in the-->
        <!--        paper are present in the paper or the Supplementary Materials. Other materials can be found at-->
        <!--        https://github.com/leggedrobotics/learning_locomotion_over_challening_terrain_supplementary.-->
    </p>
</div>